# -*- coding: utf-8 -*-
"""proj2_part5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_hAql3tNb5jUL1ii7N_ciW98EdRlr4CH

# Section 5: Cleaning, Processing, Analysis

# Dependencies

Datasets used:
*   unique_research_outputs.csv (Part 3)
*   Researchers 2024 (Git)

Dependencies listed below:
"""

import pandas as pd
import numpy as np
import plotly.express as px
import networkx as nx
import requests
from io import BytesIO
import re
import ast
import seaborn as sns
import matplotlib.pyplot as plt
from bertopic import BERTopic
import nltk
from nltk.corpus import stopwords
import string
import statsmodels.formula.api as smf # for multiple regression
import umap # for bert
import hdbscan # for bert
import unittest

"""# Defining Functions"""

def load_data(filename):
  """
    Load data from a CSV file using pandas
    Args:
    filename: Enter filename as str (include quotations)
  """
  try:
    df = pd.read_csv(filename)
    return df
  except FileNotFoundError: # important to add to deal with path errors
    print("File not found. Check file path.")
  except csv.Error as e: # csv related errors
    print(f"CSV error: {e}")
  except Exception as e: # all other errors
    print(f"An unexpected error occurred: {e}")


def load_data_from_url(url):
    """
    Loads csv/xlsx data from a URL using pandas, requests, and BytesIO
    Args:
    filename: Enter filename as str (include quotations)
    """
    # Ensure we're using the raw file URL
    if "blob" in url:
        url = url.replace("blob/", "raw/")
    # Download the file
    response = requests.get(url)
    if response.status_code == 200: # if successfully dowloaded
        content = BytesIO(response.content)
        file_extension = url.split('.')[-1].lower() # extract extensions
        # Read file based on extension
        if file_extension in ['xls', 'xlsx']: # if xls
            return pd.read_excel(content, engine="openpyxl" if file_extension == "xlsx" else "xlrd") # use read_excel
        elif file_extension == 'csv': # if csv
            return pd.read_csv(content)
        else:
            raise ValueError(f"Unsupported file format: {file_extension}")
    else:
        raise ValueError(f"Failed to download the file. Status code: {response.status_code}")


# Download stopwords if not already
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)

def clean_text(text):
    """
    Cleans text by removing stopwords and punctuation
    Args:
    filename: enter text as str (include quotations)
    """
    words = text.split()
    return ' '.join(word for word in words if word.lower() not in stop_words and word not in punctuation)

"""# Data Ingestion and Processing"""

# unique data from API
df=load_data('part3/unique_research_outputs.csv')
# unique data from webscraping
df_webscraping =load_data('part3/unique_outputs_webscraping.csv')
# 2024 data compiled earlier
data_24 =load_data_from_url("https://github.com/dingkaihua/fsrdc-external-census-projects/blob/master/ResearchOutputs.xlsx")

# checking missing values
print(df.isnull().sum())

# checking missing values
print(df_webscraping.isnull().sum())

# checking missing values in 2024 dataset
print(data_24.isnull().sum())

if len(data_24) == 1735:
    print(f"Validation passed: The dataset contains {len(data_24)} rows.")
else:
    print(f"Validation failed: The dataset contains {len(data_24)} rows, expected 1735.")

df.head(n=3)

df_webscraping.head(n=3)

data_24.head(n=3)

# checking structure of all datasets
column_names_list = df.columns.to_list()
print(column_names_list)

# checking structure of all datasets
column_names_list = df_webscraping.columns.to_list()
print(column_names_list) # missing col names such as location etc

# checking structure of all 2024 datasets
column_names_list = data_24.columns.to_list()
print(column_names_list)

duplicate_count = df.duplicated().sum()
print(f"Number of duplicate rows in unique dataset: {duplicate_count}")

# dropping duplicates
df = df.drop_duplicates()

duplicate_count = df_webscraping.duplicated().sum()
print(f"Number of duplicate rows in unique dataset: {duplicate_count}")

df.head(n=4)

# exploring duplicates:
df[df.duplicated(subset=['title', 'publication_date', 'authors',	'topics',	'source_display_name', 'abstract','location', 'type_crossref'])].sort_values(by=['title', 'publication_date']).head(n=5)

# sample duplicate output
df[df["title"] == "A Comparative Efficiency Analysis of Cooperative and Non-cooperative Dairy Manufacturing Firms"]

# drop duplicates across all cols i.e. 1 will be dropped
df = df.drop_duplicates()
df.shape

# drop duplicates across subset cols i.e. 285 will be dropped
df = df.drop_duplicates(subset=['title', 'publication_date', 'abstract','authors','source_display_name',	'topics',	'location', 'type_crossref'])
df.shape

df_webscraping.head(n=2)

# no duplicates based on selected col in webscraped data:
df_webscraping[df_webscraping.duplicated(subset=['title', 'publication_date', 'authors',	'topics', 'abstract','type_crossref'])].sort_values(by=['title', 'publication_date']).head(n=2)

# Check for duplicate rows based on all columns
duplicate_count = data_24.duplicated().sum()
print(f"Number of duplicate rows in 2024 dataset: {duplicate_count}")

data_24.head(n=3)

# Check for duplicate rows based on selected columns
duplicate_count = data_24.duplicated(subset=['OutputTitle', 'OutputType',	'ProjectRDC',	'OutputVenue','ProjectPI', 'OutputYear']).sum()
print(f"Number of duplicate rows in 2024 dataset: {duplicate_count}")

# inspecting
data_24[data_24.duplicated(subset=['OutputTitle', 'OutputType',	'ProjectRDC',	'OutputVenue','ProjectPI', 'OutputYear'])]

# drop duplicates across subset cols
data_24 = data_24.drop_duplicates(subset=['OutputTitle', 'OutputType',	'ProjectRDC',	'OutputVenue','ProjectPI', 'OutputYear'])
data_24.shape

"""# Cleaning

## API Dataset
"""

# creating subset of unique scraped dataset
df_subset = df.loc[:,["title", "year", "publication_date", "authors","abstract", "topics", "type_crossref",
 "location", "cited_by_count"]]

# Renaming variables in unique scraped dataset:
df_subset = df_subset.rename(columns={"title": "OutputTitle", "year": "OutputYear",
                                            "publication_date": "PublicationDate",
                                            "authors": "ProjectPI", "topics": "Keywords", "type_crossref": "OutputType",
                                            "location": "ProjectRDC", "abstract" : "Abstract",
                                            "cited_by_count": "CiteCount" })

df_subset.head(n=3)

# cleaning ProjectPI: removing quotes
df_subset['ProjectPI'] = df_subset['ProjectPI'].apply(lambda x: set(name.strip("'\"") for name in ast.literal_eval(x)))

# count of authors
max_length = df_subset['ProjectPI'].apply(len).max()
print("Maximum set length:", max_length)

# clean authors storing each author separately
for i in range(48):
    col_name = f'Author_{i+1}'
    df_subset[col_name] = df_subset['ProjectPI'].apply(
        lambda x: sorted(x)[i] if isinstance(x, set) and len(x) > i else None
    )

df_subset.head(n=3)

df_subset["OutputType"].value_counts()

"""Creating categories based on the output type: reports, reviews, editorial were randomly checked. Reports are NBER reports i.e. Working papers."""

type_mapping = {
    'article': 'Journal Article Publication',
    'preprint': 'Journal Article Publication',
    'report': 'Working Paper',
    'dataset': 'Dataset',
    'book-chapter': 'Book',
    'book': 'Book',
    'dissertation': 'Graduate Research Output',
    'other': 'Other Publication',
    'paratext': 'Other Publication',
    'review': 'Other Publication',
    'editorial': 'Other Publication',
    'letter': 'Other Publication',
    'JA': 'Journal Article Publication',
    'SW': 'Software',
    'WP': 'Working Paper',
    'DS': 'Dataset',
    'BC': 'Book',
    'DI': 'Graduate Research Output',
    'MT': 'Graduate Research Output',
    'RE': 'Other Publication',
    'BG': 'Other Publication',
    'MI': 'Other Publication',
    'TN': 'Other Publication'
}

# Apply the mapping to create a new column
df_subset['OutputType_Clean'] = df_subset['OutputType'].map(type_mapping)

df_subset['OutputType_Clean'].value_counts()

"""## Webscraped Data"""

df_webscraping.head(n=2)

# creating subset of scraped dataset
df_webscraping_subset = df_webscraping.loc[:,["title", "year", "publication_date", "authors","abstract", "topics", "type_crossref", "cited_by_count"]]
# adding missing column
df_webscraping_subset["location"] = "No location available"

# Renaming variables in scraped dataset:
df_webscraping_subset = df_webscraping_subset.rename(columns={"title": "OutputTitle", "year": "OutputYear",
                                            "publication_date": "PublicationDate",
                                            "authors": "ProjectPI", "topics": "Keywords", "type_crossref": "OutputType",
                                            "location": "ProjectRDC", "abstract" : "Abstract",
                                            "cited_by_count": "CiteCount" })

# cleaning ProjectPI: removing quotes
df_webscraping_subset['ProjectPI'] = df_webscraping_subset['ProjectPI'].apply(lambda x: set(name.strip("'\"") for name in ast.literal_eval(x)))

# count of authors
max_length = df_webscraping_subset['ProjectPI'].apply(len).max()
print("Maximum set length:", max_length)

# clean authors storing each author separately
for i in range(48): # 48 set because i want to concatenate it with api dataset
    col_name = f'Author_{i+1}'
    df_webscraping_subset[col_name] = df_webscraping_subset['ProjectPI'].apply(
        lambda x: sorted(x)[i] if isinstance(x, set) and len(x) > i else None
    )

df_webscraping_subset["OutputType"].value_counts()

# Apply the mapping to create a new column
df_webscraping_subset['OutputType_Clean'] = df_webscraping_subset['OutputType'].map(type_mapping)

df_webscraping_subset.head(n=3)

"""## 2024 Dataset"""

# replacing missing project years with end year
data_24['OutputYear'] = data_24.apply(
    lambda row: row['ProjectEndYear'] if pd.isnull(row['OutputYear']) else row['OutputYear'],
    axis=1)

# creating subset of 2024 dataset
df_subset_24 = data_24.loc[:,["ProjectTitle", "ProjectRDC", "OutputTitle", "OutputType", "OutputYear"]]

df_subset_24.head(n=3)

# convert year to integer
df_subset_24["OutputYear"] = df_subset_24["OutputYear"].astype(int)

"""```
BC = book chapter
BG = blog
DI = Ph.D. dissertation
DS = dataset
JA = journal article
MI = mimeo
MT = Master's thesis
RE = report
SW = software
TN = CES Technical Note
WP = working paper
```

"""

df_subset_24['OutputType'].value_counts()

# Apply the mapping to create a new column
df_subset_24['OutputType_Clean'] = df_subset_24['OutputType'].map(type_mapping)

df_subset_24

df_subset_24['OutputType_Clean'].value_counts()

"""# Descriptives

Identification of overlap between webscraped and API data based on title, publication date, output type and keywords
"""

# merging the two datasets to identify the rows not in df_subset
anti_join_df = df_webscraping_subset.merge(df_subset, on=['OutputTitle','PublicationDate','Keywords','OutputType_Clean'], how='left', indicator=True)
# Filter only rows that exist in df_webscraping_subset but NOT in df_subset
anti_join_df = anti_join_df[anti_join_df['_merge'] == 'left_only'].drop(columns=['_merge'])

anti_join_df.shape

df_webscraping_subset.shape # all unqiue

# checking with samples
df_subset[df_subset["OutputTitle"] == "The Impact of Industrial Opt-Out from Utility Sponsored Energy Efficiency Programs"]

df_subset[df_subset["OutputTitle"] == "Output Market Power and Spatial Misallocation"]

df_webscraping_subset.head(n=3)

df_webscraping_subset["Data Source"] = "New"

df_subset["Data Source"] = "New"

df_subset.head(n=3)

df_subset_24["Data Source"] = "FSRDC-verified 2024"

df_subset_24.head(n=3)

# joining API and webscraped
df_subset = pd.concat([df_subset, df_webscraping_subset], ignore_index=True)

df_subset['PublicationDate_clean'] = pd.to_datetime(df_subset['PublicationDate'], errors='coerce')

# Extract the year and assign it to 'OutputYear'
df_subset['OutputYear_clean'] = df_subset['PublicationDate_clean'].dt.year

df_subset.loc[:,['OutputYear_clean', 'PublicationDate_clean','PublicationDate', 'OutputYear']]

df_subset['OutputYear_clean'] = df_subset['OutputYear_clean'].fillna(df_subset['OutputYear'])

df_subset.loc[:,['OutputYear_clean', 'PublicationDate_clean','PublicationDate', 'OutputYear']]

df_subset = df_subset.rename(columns={"OutputYear": "year"})

df_subset = df_subset.rename(columns={"OutputYear_clean": "OutputYear"})

# joining the 2024 with new
graph1 = pd.concat([df_subset_24.loc[:, ["ProjectRDC", "OutputTitle","OutputType_Clean","OutputYear","Data Source"]], df_subset.loc[:, ["ProjectRDC", "OutputTitle","OutputType_Clean","OutputYear", "Data Source"]]], ignore_index=True)

graph1.head(n=3)

# comparing counts across years new vs old
graph1_1 =  graph1.groupby(["OutputYear","Data Source"]).size().reset_index(name="Count")

graph1_1.head(n=3)

# program started in 1989 so dropping before those years
graph1_1 = graph1_1[graph1_1["OutputYear"] > 1988]

sns.set_style("white")
sns.lineplot(data=graph1_1, x='OutputYear', y='Count', hue='Data Source', marker='o')

# Add titles and labels
plt.title('Counts Over Years by Data Source')
plt.xlabel('Year')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

graph1_1[graph1_1["Data Source"] == "New" ].sort_values(by="Count", ascending=False)

graph1_1.sort_values(by="OutputYear", ascending=False)

graph1_1[graph1_1["OutputYear"] == 2009]

table1 = graph1.groupby(["Data Source", "OutputType_Clean"]).size().reset_index(name="Count")

table1 =table1.pivot(index="OutputType_Clean", columns="Data Source")

table1.head(n=3)

# remove the first level of labels
table1.columns = table1.columns.droplevel(0)

# replace NaN with 0
table1 = table1.fillna(0)

table1

table1["Percent-FSRDC-verified 2024"] = ((table1["FSRDC-verified 2024"])/(table1['FSRDC-verified 2024'].sum()))*100
table1["Percent-New"] = ((table1["New"])/(table1['New'].sum()))*100

table1["Difference"] = table1["Percent-New"] - table1["Percent-FSRDC-verified 2024"]

table1.loc[:,["Percent-FSRDC-verified 2024", "Percent-New", "Difference"]]

sns.set_style("white")

sns.barplot(data=table1, y='OutputType_Clean', x='Difference')

plt.xlabel("Percent Difference")
plt.ylabel("")
plt.title("Comparison of New Dataset vs FSRDC-verified 2024")
plt.yticks(fontsize=8)

plt.tight_layout()

plt.show()

table2_2 =graph1.groupby(["Data Source", "ProjectRDC", "OutputYear"]).size().reset_index(name="Count")

table2_2_new  = table2_2[table2_2["Data Source"]=="New"]

table2_2_new[table2_2_new['ProjectRDC'] == "UCLA"].sort_values(by="Count", ascending=False)

sns.set_theme(style="ticks", palette="pastel")

sns.boxplot(x='Count', y='ProjectRDC', data=table2_2_new, color= "m")
sns.despine(offset=10, trim=True)
# Reduce the size of y-axis labels
plt.yticks(fontsize=8)  # Adjust the font size of the y-axis labels

# Show the plot
plt.show()

table2_2_old  = table2_2[table2_2["Data Source"]!="New"]

sns.set_theme(style="ticks", palette="pastel")

sns.boxplot(x='Count', y='ProjectRDC', data=table2_2_old, color= "g")
sns.despine(offset=10, trim=True)
# Reduce the size of y-axis labels
plt.yticks(fontsize=8)  # Adjust the font size of the y-axis labels

# Show the plot
plt.show()

sns.set_theme(style="ticks", palette="pastel")

# Draw a nested boxplot to show bills by day and time
sns.boxplot(x="Count", y="ProjectRDC",
            hue="Data Source", palette=["m", "g"],
            data=table2_2)
sns.despine(offset=10, trim=True)
plt.yticks(fontsize=8)  # Adjust the font size of the y-axis labels

# Show the plot
plt.show()

"""# Analysis

## Topic Modelling
"""

df_subset.head(n=3)
# replacing missing abstracts with titles
df_subset['Abstract'] = df_subset.apply(
    lambda row: row['OutputTitle'] if row['Abstract'] == "No abstract available" else row['Abstract'],
    axis=1)

# removing punctuation and stop words from abstract
df_subset['Abstract_clean'] = df_subset['Abstract'].apply(clean_text)
df_subset_1989 = df_subset[df_subset['OutputYear'] > 1988]

df_subset_1989.shape

# no need to tokenize
random_seed = 14678 # to esnure same topics identified

# Set UMAP's random state for reproducibility
umap_model = umap.UMAP(random_state=random_seed)

# Initialize the HDBSCAN
hdbscan_model = hdbscan.HDBSCAN()

# Initialize the BERTopic model with UMAP and HDBSCAN
model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, nr_topics=5)

topics, probs = model.fit_transform(df_subset_1989['Abstract_clean'])
# Add topics back to the DataFrame
df_subset_1989['Topic'] = topics

df_subset_1989['Topic'].value_counts()

model.visualize_topics()

topics = model.get_topics()
# Display the top words for each topic
for topic, words in topics.items():
    print(f"Topic {topic}:")
    for word, weight in words:
        print(f"  {word}: {weight}")
    print()

df_subset_1989['OutputYear']

topics_over_time = model.topics_over_time(df_subset_1989['Abstract_clean'], df_subset_1989['OutputYear'],datetime_format= "%Y", nr_bins=36)

df_subset_1989["OutputYear"].value_counts().sort_index()

topics_over_time['Timestamp'].value_counts()

topics_over_time

custom_labels = {
    -1: "Rural Infrastructure & Employment Data",
    0: "Demographic Data & Household Structures",
    1: "Growth, Firm and Productivity",
    2: "Efficiency, Productivity and Economic Growth ",
    3: "Human Capital, Labor Force and Productivity"

}

topics_over_time['TopicsClean'] = topics_over_time['Topic'].map(custom_labels)

topics_over_time.head(n=10) # fractional format of date

table3 = topics_over_time.loc[:, ["Frequency", "Timestamp"]]
table3 = table3.groupby(["Timestamp"]).sum("Frequency")

table3= table3.rename(columns={"Frequency": "TotalCount"})
table3.head(n=3)

# merge total count
table3 = pd.merge(topics_over_time, table3, on="Timestamp", how='left', indicator=True)
table3.head(n=3)

table3["Percent"] = (table3["Frequency"]/table3["TotalCount"])*100

table3.head(n=3)

table3.sort_values(by="Timestamp", ascending=False)

table3[table3["Timestamp"] > 2007]

sns.set_style("white")
sns.lineplot(data=table3, x='Timestamp', y='Percent', hue='TopicsClean', marker='o')

# Add titles and labels
plt.title('Evolution of Topics Across Years')
plt.xlabel('')
plt.ylabel('Pecentage')
plt.xticks(rotation=45)
plt.legend(
    title='',
    loc='upper center',
    bbox_to_anchor=(0.5, -0.15),
    ncol=4,  # number of columns in the legend
    prop={'size': 9},
    frameon=False
)

#plt.legend(title='Topics', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., prop={'size': 7})

plt.tight_layout()
plt.show()

df_subset_1989.head(n=3)

test= df_subset_1989.groupby(["OutputYear", "Topic"]).agg({'CiteCount': ['sum']}).reset_index()
test.columns = test.columns.droplevel(1)
test['TopicsClean'] = test['Topic'].map(custom_labels)

df_subset_1989[df_subset_1989["OutputYear"] == 2024]

# Create the scatter plot, coloring by topic
sns.scatterplot(x='OutputYear', y='CiteCount', hue='TopicsClean', data=test)
# Show the plot
plt.show() #

sns.set_style("white")

sns.boxplot(x='CiteCount', y='TopicsClean', data=test)

test[test["CiteCount"] > 9000]

"""## Multivariate Regression Analysis"""

df_subset_1989['TopicsClean'] = df_subset_1989['Topic'].map(custom_labels)

df_subset_1989.head(n=3)

df_subset_1989['PublicationDate'] = pd.to_datetime(df_subset_1989['PublicationDate'])
# Specify reference date
reference_date = pd.Timestamp('2025-03-30')
# Calculate days since publication
df_subset_1989['days_since_publication'] = (reference_date - df_subset_1989['PublicationDate']).dt.days

print(df_subset_1989.isnull().sum())

df_subset_1989.head(n=3)

sns.lmplot(x='days_since_publication', y='CiteCount', data=df_subset_1989, hue='TopicsClean')
plt.show()

model_ols = smf.ols('CiteCount ~ days_since_publication+C(TopicsClean)+days_since_publication*C(TopicsClean)', data=df_subset_1989).fit()

print(model_ols.summary())

"""# Additional Analysis: Network"""

class ResearchOutput:
  """
  Create object ResearchOutput storing title, year, agency and topics
  Defines equality cases for testing purposes
  """
  def __init__(self, title, year, agency, topics, keyword, author):  # constructor
    self.title = title
    self.year = year
    self.agency = agency
    self.topics = topics
    self.keyword = keyword
    self.author = author

  def __repr__(self):
    return f"ResearchOutput({self.title},{self.year})"

  def __eq__(self, other): # important for unit testing later
    if isinstance(other, ResearchOutput):
      return self.title == other.title and self.year == other.year and self.keyword == self.keyword and self.agency == self.agency and self.author == self.author # Compare the title and year attributes for equality
    return False
  def __hash__(self):
    return hash((self.title, self.year)) # this is imp as we want use them as dictionary keys later

outputs = [] # initialize empty list
for index, row in df_subset_1989.iterrows():
  output = ResearchOutput(row['OutputTitle'], row['OutputYear'], row['ProjectRDC'],row['TopicsClean'],  row['Keywords'], row['ProjectPI']) # using class defined earlier
  outputs.append(output) # add to list

def shareCommonAttribute(r1, r2):
    """
    Function to identify links between research outputs
    Args: two research output objects
    Returns: True if research outputs share common attributes, False otherwise
    """
    if r1.agency.lower() == r2.agency.lower() and r1.topics.lower() == r2.topics.lower(): # check link between agency
        return True
    return False # if no common attributes are found, return False

def create_graph(name_dataset):
  """
  Function to create graph linking research outputs with common agency/keyword
  Args: name of list with each research output stored  as object
  Returns: dict object with nodes as key and edges as values
  """
  G = {} # intialize graph as empty dictionary
  for r in name_dataset:
    G[r] = [] # empty list for values for nodes
  for i in range(len(name_dataset)): # start loop begining from first item
    for j in range(i + 1, len(name_dataset)): # starting from second item
      r1 = name_dataset[i] # storing the info for 1 element
      r2 = name_dataset[j] # storing the info for 2 element
      if shareCommonAttribute(r1,r2): # identifying if they have any common agency/keyword
        G[r1].append(r2)  # adding links
        G[r2].append(r1)  # adding links
  return G

G = create_graph(outputs)
len(G)

# Check unique outputs i.e. outputs with different titles, keywords and years
unique_outputs = set() # initalize empty set
for r in outputs:
  unique_outputs.add(r) # add items to set

print(f"The dataset contains {len(unique_outputs)} unique research outputs i.e. title, year, agency, topics, keyword, author.")

import networkx as nx
import matplotlib.pyplot as plt

# Step 1: Create the graph
graph = nx.Graph()
for node in G.keys():
    graph.add_node(node)

# Step 2: Add edges from the dictionary
for node, neighbors in G.items():
    for neighbor in neighbors:
        graph.add_edge(node, neighbor)

# Step 3: Find connected components
connected_components = list(nx.connected_components(graph))

# Step 3: Identify connected components
components = list(nx.connected_components(graph))

# Step 4: Create a dictionary of labels where the key is the node and the value is the component size
labels_to_display = {}
for component in components:
    component_size = len(component)  # Size of the current component
    # Choose a representative node from the component (e.g., first node)
    representative_node = next(iter(component))
    labels_to_display[representative_node] = str(component_size)

# Step 4: Assign colors to nodes based on connected components
node_colors = []
for node in graph.nodes():
    for idx, component in enumerate(connected_components):
        if node in component:
            node_colors.append(f"C{idx}")  # Color by component index

graph.remove_edges_from(nx.selfloop_edges(graph))

# Step 5: Visualize the graph with nodes colored by connected component
plt.figure(figsize=(8, 8))
pos = nx.spring_layout(graph)  # Positions for the nodes

# Draw the graph with different colors for different components
nx.draw(graph, pos, with_labels=False, node_size=10, node_color=node_colors, font_size=10, font_weight='bold', edge_color="gray", alpha=0.7)
nx.draw_networkx_labels(graph, pos, labels=labels_to_display, font_size=10, font_weight='bold')

# Title and show the plot
plt.title("Network with Connected Components")
plt.show()

# Optional: Print the connected components
print("Connected components:")
for idx, component in enumerate(connected_components):
    print(f"Component {idx + 1}: {component}")

print(f"Number of nodes: {graph.number_of_nodes()}")
print(f"Number of edges: {graph.number_of_edges()}")

labels_to_display

df_subset_1989[df_subset_1989["OutputTitle"] == "Comparisons of administrative record rosters to census self-responses and NRFU household member responses1"]

df_subset_bos = df_subset_1989[df_subset_1989["ProjectRDC"] == "Boston"]
df_subset_bos= df_subset_bos[df_subset_bos["TopicsClean"] == "Demographic Data & Household Structures"]

df_subset_bos["CiteCount"].mean()

"""# Testing

## Testing Functions

### Processing and Cleaning
"""

data_test = {
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'city': ['New York', 'Los Angeles', 'Chicago']
}

data_test = pd.DataFrame(data_test)

data_test.to_csv('sample_data_file.csv', index=False)

load_data("imaginary.csv")

test2 = load_data("sample_data_file.csv")

# dataset processing function
assert data_test.equals(test2)

# text cleaning
assert clean_text("I am a cocunut") == 'cocunut' # removing stop words
assert clean_text("I am a") == '' # all stop words

assert clean_text("I am the queen of the world") == 'queen world' # sandwiched stop words

"""### Graph"""

outputs = [] # initialize empty list
for index, row in df_subset_1989.iterrows():
  output = ResearchOutput(row['OutputTitle'], row['OutputYear'], row['ProjectRDC'],row['TopicsClean'],  row['Keywords'], row['ProjectPI']) # using class defined earlier
  outputs.append(output) # add to list

df_subset_1989.loc[:,['OutputTitle','OutputYear','ProjectRDC','TopicsClean','Keywords','ProjectPI']].head(n=3)

# Test 1: Only link is rdc
research_outputs_test1 = [ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"),
                          ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"),
                          ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "Boston", "Firm Growth","technology; innovation; exports",  "{D. Stigler Joseph}")]

# Test 2: Only link is topic
research_outputs_test2 = [ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"),
                          ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"),
                          ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "UCLA", "Economic Growth","technology; innovation; exports",  "{D. Stigler Joseph}")]

# Test 3: No Link
research_outputs_test3 = [ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"),
                          ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"),
                          ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "UCLA", "Firm Growth","technology; innovation; exports",  "{D. Stigler Joseph}")]


# Test 4: Both linked
research_outputs_test4 = [ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"),
                          ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"),
                          ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "Boston", "Gender and Growth","technology; innovation; exports",  "{D. Stigler Joseph}")]

# Using Test 1: Only link is rdc
graph_result1= {ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"): [],
  ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"): [],
 ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "Boston", "Firm Growth","technology; innovation; exports",  "{D. Stigler Joseph}"): []}



# Using Test 2:

graph_result2= {ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"): [],
  ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"): [],
 ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "UCLA", "Economic Growth","technology; innovation; exports",  "{D. Stigler Joseph}"): []}


# Using Test 3

graph_result3= {ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"): [],
  ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"): [],
 ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "UCLA", "Firm Growth","technology; innovation; exports",  "{D. Stigler Joseph}"): []}

# Using Test 4:

graph_result4= {ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}"): [ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "Boston", "Gender and Growth","technology; innovation; exports",  "{D. Stigler Joseph}")],
  ResearchOutput("The Role of Trade Liberalization in Economic Growth", "2005", "Michigan", "Economic Growth", "Employment and Welfare Studies", "{Aaron Stromer}"): [],
 ResearchOutput("Technological Innovation and Export Competitiveness", "2006", "Boston", "Gender and Growth","technology; innovation; exports",  "{D. Stigler Joseph}"): [ResearchOutput("Labor Market Returns", "2002", "Boston", "Gender and Growth", "Cooperative Studies and Economics", "{Aaron Flaaen, Yue Maggie Zhou}")]}

research_outputs_test1[2]

class TestGraph(unittest.TestCase):

    def test_shareCommonAttribute(self):
        '''Test case function for shareCommonAttributes'''
        result = []
        result.append(shareCommonAttribute(research_outputs_test1[0],research_outputs_test1[1]))
        result.append(shareCommonAttribute(research_outputs_test1[0],research_outputs_test1[2]))
        result.append(shareCommonAttribute(research_outputs_test4[0],research_outputs_test4[2]))
        expected = [False, False, True]
        self.assertEqual(result, expected)
    def test_create_graph_1(self):
        '''Test case function for graph: with Test Case 1 '''
        result = create_graph(research_outputs_test1)
        expected = graph_result1
        self.assertEqual(result, expected)
    def test_create_graph_2(self):
        '''Test case function for graph: with Test Case 2 '''
        result = create_graph(research_outputs_test2)
        expected = graph_result2
        self.assertEqual(result, expected)
    def test_create_graph_3(self):
        '''Test case function for graph: with Test Case 3  '''
        result = create_graph(research_outputs_test3)
        expected = graph_result3
        self.assertEqual(result, expected)

    def test_create_graph_4(self):
        '''Test case function for graph: with Test Case 4 '''
        result = create_graph(research_outputs_test4)
        expected = graph_result4
        self.assertEqual(result, expected)

if __name__ == '__main__':
    unittest.main(argv=[''], verbosity=2, exit=False)
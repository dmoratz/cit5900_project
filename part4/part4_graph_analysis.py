# -*- coding: utf-8 -*-
"""part4_graph_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9dXbGVvZtuFFsfXWh41Pd3d8j_3Cs8I
"""

import pandas as pd
import networkx as nx
from ast import literal_eval
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
from community import community_louvain
import re
import itertools

def clean_author_name(author_str):
    """Clean individual author names by removing extra characters and normalizing."""
    author_str = re.sub(r'[\{\}\'"]', '', author_str).strip()
    author_str = re.sub(r'\b([A-Z])\.\s*', r'\1 ', author_str)
    return author_str

def process_authors_field(authors_str):
    """Process the authors field which may be a string representation of a set."""
    if pd.isna(authors_str):
        return []

    try:
        authors = literal_eval(authors_str)
        if isinstance(authors, (set, list)):
            return [clean_author_name(a) for a in authors]
        elif isinstance(authors, str):
            return [clean_author_name(authors)]
    except (SyntaxError, ValueError):
        clean_str = authors_str.strip('{}')
        authors = [a.strip() for a in clean_str.split(',') if a.strip()]
        return [clean_author_name(a) for a in authors]

    return []

def load_and_preprocess_data(filepath):
    """Load and preprocess the research outputs dataset."""
    df = pd.read_csv(filepath)

    # Remove duplicate titles (keeping the first occurrence)
    df = df.drop_duplicates(subset=['title'], keep='first')

    # Process authors field
    df['authors'] = df['authors'].apply(process_authors_field)

    # Matched datasets
    df['matched_datasets'] = df['matched_dataset_terms'].str.split('; ') if 'matched_dataset_terms' in df.columns else [[] for _ in range(len(df))]

    # Topics
    df['topics'] = df['topics'].str.split('; ') if 'topics' in df.columns else [[] for _ in range(len(df))]

    # Affiliations
    if 'affiliations' in df.columns:
        df['affiliations'] = df['affiliations'].fillna('').apply(lambda x: x.split('; ') if x else [])
    else:
        df['affiliations'] = [[] for _ in range(len(df))]

    # Location
    df['location'] = df['location'] if 'location' in df.columns else None

    # Create a unique identifier for each output
    df['output_id'] = df['doi'].fillna(df['title'])

    return df

def construct_research_graph(df):
    """Construct a graph where nodes are research outputs and edges represent shared attributes."""
    G = nx.Graph()

    # Dictionary to store attributes for each output
    output_attributes = defaultdict(dict)

    # First pass: add all output with their attributes
    for _, row in df.iterrows():
        output_id = row['output_id']

        if pd.isna(output_id):
            continue

        # Add output node with attributes
        attributes = {
            'type': 'output',
            'title': str(row['title']) if pd.notnull(row['title']) else "Untitled",
            'year': row['year'] if pd.notnull(row['year']) else None,
            'authors': row['authors'],
            'datasets': row['matched_datasets'] if isinstance(row['matched_datasets'], list) else [],
            'topics': row['topics'] if isinstance(row['topics'], list) else [],
            'affiliations': row['affiliations'] if isinstance(row['affiliations'], list) else [],
            'location': row['location'] if pd.notnull(row['location']) else None
        }

        G.add_node(output_id, **attributes)
        output_attributes[output_id] = attributes

    # Second pass: create edges based on shared attributes
    for output1, output2 in itertools.combinations(output_attributes.keys(), 2):
        attrs1 = output_attributes[output1]
        attrs2 = output_attributes[output2]

        shared_attributes = {
            'shared_authors': len(set(attrs1['authors']) & set(attrs2['authors'])),
            'shared_datasets': len(set(attrs1['datasets']) & set(attrs2['datasets'])),
            'shared_topics': len(set(attrs1['topics']) & set(attrs2['topics'])),
            'shared_affiliations': len(set(attrs1['affiliations']) & set(attrs2['affiliations'])),
            'same_location': 1 if attrs1['location'] == attrs2['location'] and attrs1['location'] is not None else 0
        }

        total_similarity = sum(shared_attributes.values())

        if total_similarity > 0:
            G.add_edge(output1, output2, weight=total_similarity, **shared_attributes)

    return G

def calculate_network_metrics(G):
    """Calculate various network metrics."""
    metrics = {}

    metrics['num_nodes'] = G.number_of_nodes()
    metrics['num_edges'] = G.number_of_edges()

    # Centrality measures
    metrics['degree_centrality'] = nx.degree_centrality(G)
    metrics['betweenness_centrality'] = nx.betweenness_centrality(G, weight='weight') if G.number_of_edges() > 0 else {}
    metrics['closeness_centrality'] = nx.closeness_centrality(G, distance='weight') if G.number_of_edges() > 0 else {}

    # Clustering - handle case when no edges or all clustering coefficients are zero
    try:
        metrics['average_clustering'] = nx.average_clustering(G, weight='weight')
    except ZeroDivisionError:
        metrics['average_clustering'] = 0.0

    # Community detection
    if G.number_of_edges() > 0:
        partition = community_louvain.best_partition(G, weight='weight')
        nx.set_node_attributes(G, partition, 'community')
        metrics['communities'] = Counter(partition.values())
    else:
        metrics['communities'] = {}

    return metrics, G

def visualize_graph(G, max_nodes=10):
    """Visualize the graph with communities highlighted, showing node titles and edge attributes."""
    if len(G.nodes()) > max_nodes:
        print(f"Graph too large ({len(G.nodes())} nodes), visualizing a sample")
        nodes = list(G.nodes())
        nodes_sample = nodes[:max_nodes]
        G = G.subgraph(nodes_sample)

    # Get community information if available
    communities = nx.get_node_attributes(G, 'community')
    if communities:
        node_color = [communities[n] for n in G.nodes()]
    else:
        node_color = 'blue'

    plt.figure(figsize=(20, 20))
    pos = nx.spring_layout(G, weight='weight', k=0.5, iterations=100)

    # Draw nodes colored by community
    nx.draw_networkx_nodes(G, pos, node_color=node_color, cmap=plt.cm.tab20, node_size=800, alpha=0.8)

    # Draw edges with transparency based on weight
    edges = G.edges(data=True)
    edge_weights = [d.get('weight', 1) for _, _, d in edges]
    max_weight = max(edge_weights) if edge_weights else 1
    edge_alphas = [0.1 + 0.9*(w/max_weight) for w in edge_weights]

    # Draw edges with labels showing shared attributes
    for i, (u, v, d) in enumerate(G.edges(data=True)):
        # Create edge label showing all shared attributes
        edge_label_parts = []
        if d.get('shared_authors', 0) > 0:
            edge_label_parts.append(f"Authors: {d['shared_authors']}")
        if d.get('shared_datasets', 0) > 0:
            edge_label_parts.append(f"Datasets: {d['shared_datasets']}")
        if d.get('shared_topics', 0) > 0:
            edge_label_parts.append(f"Topics: {d['shared_topics']}")
        if d.get('shared_affiliations', 0) > 0:
            edge_label_parts.append(f"Affiliations: {d['shared_affiliations']}")
        if d.get('same_location', 0) > 0:
            edge_label_parts.append("Same Location")

        edge_label = "\n".join(edge_label_parts)

        # Draw the edge with label
        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], alpha=edge_alphas[i], width=2, edge_color='gray')

        # Add edge label at midpoint
        if edge_label:  # Only add label if there are shared attributes
            x = (pos[u][0] + pos[v][0]) / 2
            y = (pos[u][1] + pos[v][1]) / 2
            plt.text(x, y, edge_label,
                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'),
                    fontsize=8, ha='center', va='center')

    # Draw node labels with titles
    node_labels = {n: f"{G.nodes[n].get('title', n)[:30]}\n(Year: {G.nodes[n].get('year', 'N/A')})"
                  for n in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10, font_weight='bold')

    plt.title("Research Outputs Network\nNodes colored by community, edges show shared attributes", fontsize=14)
    plt.axis('off')

    # Add legend for community colors if communities exist
    if communities:
        unique_communities = set(communities.values())
        handles = [plt.Line2D([0], [0], marker='o', color='w',
                             markerfacecolor=plt.cm.tab20(i), markersize=10)
                  for i in range(len(unique_communities))]
        plt.legend(handles, [f"Community {i}" for i in unique_communities],
                  loc='upper right', bbox_to_anchor=(1.1, 1.1))

    plt.tight_layout()
    plt.show()

def analyze_network(filepath):
    """Complete analysis pipeline."""
    df = load_and_preprocess_data(filepath)
    G = construct_research_graph(df)
    metrics, G = calculate_network_metrics(G)

    num_connected_components = nx.number_connected_components(G)
    print(f"The graph has {num_connected_components} connected components")

    print(f"Network with {metrics['num_nodes']} outputs and {metrics['num_edges']} connections")
    print(f"Average clustering coefficient: {metrics['average_clustering']:.3f}")

    if metrics['communities']:
        print(f"Detected {len(metrics['communities'])} communities")

    # Get top outputs by different centrality measures
    print("\nTop outputs by degree centrality (most connections):")
    for output, score in sorted(metrics['degree_centrality'].items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"{score:.3f}: {G.nodes[output].get('title', output)}")

    print("\nTop outputs by betweenness centrality (bridge outputs):")
    for output, score in sorted(metrics['betweenness_centrality'].items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"{score:.3f}: {G.nodes[output].get('title', output)}")

    visualize_graph(G)

    return G, metrics

research_graph, metrics = analyze_network('unique_research_outputs.csv')

research_graph, metrics = analyze_network('unique_outputs_webscraping.csv')

# Test data
TEST_CSV_DATA = """title,authors,year,matched_dataset_terms,doi,topics,affiliations,location
"Paper 1","{'Author A', 'Author B'}",2020,"dataset1; dataset2","doi1","topic1; topic2","affil1; affil2","loc1"
"Paper 2","{'Author B', 'Author C'}",2021,"dataset2; dataset3","doi2","topic2; topic3","affil2; affil3","loc1"
"Paper 3","{'Author D'}",2022,"dataset4","doi3","topic4","affil4","loc2"
"Duplicate Paper","{'Author E'}",2023,"dataset5","doi4","topic5","affil5","loc3"
"Duplicate Paper","{'Author E'}",2023,"dataset5","doi4","topic5","affil5","loc3"
"Paper with missing data",,,"",,,,
"""

def test_clean_author_name():
    print("Testing clean_author_name...")
    # Test various author name formats
    assert clean_author_name("A. Author") == "A Author"
    assert clean_author_name("B. C. Author") == "B C Author"
    assert clean_author_name("{Author Name}") == "Author Name"
    assert clean_author_name("'Author Name'") == "Author Name"
    assert clean_author_name('"Author Name"') == "Author Name"
    assert clean_author_name("  Author Name  ") == "Author Name"
    assert clean_author_name("") == ""
    print("clean_author_name tests passed!")

def test_process_authors_field():
    print("\nTesting process_authors_field...")
    # Test different author field formats
    print(process_authors_field("{'Author A', 'Author B'}"))
    assert process_authors_field("{'Author A', 'Author B'}") == ["Author B", "Author A"]
    assert process_authors_field("Author A, Author B") == ["Author A", "Author B"]
    assert process_authors_field("'Author A'") == ["Author A"]
    assert process_authors_field("") == []
    assert process_authors_field(None) == []
    assert process_authors_field("{Author A, Author B}") == ["Author A", "Author B"]
    assert process_authors_field("A. Author, B. Author") == ["A Author", "B Author"]
    print("process_authors_field tests passed!")

def test_construct_research_graph():
    print("\nTesting construct_research_graph...")
    # Create a test DataFrame
    data = {
        'output_id': ['1', '2', '3'],
        'title': ['Paper 1', 'Paper 2', 'Paper 3'],
        'year': [2020, 2021, 2022],
        'authors': [['Author A', 'Author B'], ['Author B', 'Author C'], ['Author D']],
        'matched_datasets': [['dataset1', 'dataset2'], ['dataset2', 'dataset3'], ['dataset4']],
        'topics': [['topic1', 'topic2'], ['topic2', 'topic3'], ['topic4']],
        'affiliations': [['affil1', 'affil2'], ['affil2', 'affil3'], ['affil4']],
        'location': ['loc1', 'loc1', 'loc2']
    }
    df = pd.DataFrame(data)

    G = construct_research_graph(df)

    # Test graph construction
    assert isinstance(G, nx.Graph)
    assert len(G.nodes) == 3
    assert len(G.edges) == 1  # Only papers 1 and 2 should be connected

    # Test node attributes
    node1 = G.nodes['1']
    assert node1['title'] == 'Paper 1'
    assert node1['year'] == 2020
    assert 'Author A' in node1['authors']

    # Test edge attributes
    edge_data = G.get_edge_data('1', '2')
    assert edge_data['weight'] > 0
    assert edge_data['shared_authors'] == 1
    assert edge_data['shared_datasets'] == 1
    assert edge_data['shared_topics'] == 1
    assert edge_data['shared_affiliations'] == 1
    assert edge_data['same_location'] == 1
    print("construct_research_graph tests passed!")

def test_calculate_network_metrics():
    print("\nTesting calculate_network_metrics...")
    # Create a simple test graph
    G = nx.Graph()
    G.add_node('1', title='Paper 1', year=2020, authors=['Author A'], datasets=['dataset1'],
               topics=['topic1'], affiliations=['affil1'], location='loc1')
    G.add_node('2', title='Paper 2', year=2021, authors=['Author A'], datasets=['dataset1'],
               topics=['topic1'], affiliations=['affil1'], location='loc1')
    G.add_edge('1', '2', weight=5, shared_authors=1, shared_datasets=1,
               shared_topics=1, shared_affiliations=1, same_location=1)

    metrics, G_with_communities = calculate_network_metrics(G)

    # Test basic metrics
    assert metrics['num_nodes'] == 2
    assert metrics['num_edges'] == 1
    assert metrics['average_clustering'] >= 0

    # Test centrality measures
    assert '1' in metrics['degree_centrality']
    assert '1' in metrics['betweenness_centrality']
    assert '1' in metrics['closeness_centrality']

    # Test community detection
    assert metrics['communities']  # Should have at least one community
    assert 'community' in G_with_communities.nodes['1']
    print("calculate_network_metrics tests passed!")

def test_empty_graph_handling():
    print("\nTesting empty_graph_handling...")
    # Test with empty DataFrame
    empty_df = pd.DataFrame(columns=['title', 'authors', 'year', 'matched_dataset_terms', 'doi'])
    G = construct_research_graph(empty_df)
    assert len(G.nodes) == 0
    assert len(G.edges) == 0

    # Test metrics calculation on empty graph
    metrics, _ = calculate_network_metrics(G)
    assert metrics['num_nodes'] == 0
    assert metrics['num_edges'] == 0
    assert metrics['communities'] == {}
    assert metrics['degree_centrality'] == {}
    print("empty_graph_handling tests passed!")

def test_calculate_network_metrics():
    print("\nTesting calculate_network_metrics...")
    # Test with connected graph
    G = nx.Graph()
    G.add_node('1', title='Paper 1', year=2020, authors=['Author A'], datasets=['dataset1'],
               topics=['topic1'], affiliations=['affil1'], location='loc1')
    G.add_node('2', title='Paper 2', year=2021, authors=['Author A'], datasets=['dataset1'],
               topics=['topic1'], affiliations=['affil1'], location='loc1')
    G.add_edge('1', '2', weight=5, shared_authors=1, shared_datasets=1,
               shared_topics=1, shared_affiliations=1, same_location=1)

    metrics, G_with_communities = calculate_network_metrics(G)

    assert metrics['num_nodes'] == 2
    assert metrics['num_edges'] == 1
    assert metrics['average_clustering'] >= 0
    assert '1' in metrics['degree_centrality']
    assert '1' in metrics['betweenness_centrality']
    assert '1' in metrics['closeness_centrality']
    assert metrics['communities']
    assert 'community' in G_with_communities.nodes['1']

    # Test with graph with no edges
    G = nx.Graph()
    G.add_node('1', title='Paper 1')
    G.add_node('2', title='Paper 2')

    metrics, G_with_communities = calculate_network_metrics(G)

    assert metrics['num_nodes'] == 2
    assert metrics['num_edges'] == 0
    assert metrics['average_clustering'] == 0.0
    assert metrics['degree_centrality']['1'] == 0
    assert metrics['betweenness_centrality'] == {}
    assert metrics['closeness_centrality'] == {}
    assert metrics['communities'] == {}

    print("calculate_network_metrics tests passed!")

def test_visualize_graph():
    print("\nTesting visualize_graph (visual check only)...")
    # We would like to verify that it can run without errors
    G = nx.Graph()
    G.add_node('1', title='Paper 1', year=2020, community=0)
    G.add_node('2', title='Paper 2', year=2021, community=1)
    G.add_edge('1', '2', weight=1)

    # Just verify the function runs without errors
    visualize_graph(G)

    # Test with empty graph
    visualize_graph(nx.Graph())

    # Test with larger graph than max_nodes
    G = nx.complete_graph(15)
    for n in G.nodes():
        G.nodes[n]['title'] = f'Paper {n}'
        G.nodes[n]['year'] = 2000 + n
    visualize_graph(G)  # Should print message about sampling
    print("visualize_graph tests passed (no exceptions)!")

def run_all_tests():
    print("Starting all tests...\n")

    # Run all test functions
    test_clean_author_name()
    test_process_authors_field()
    test_construct_research_graph()
    test_calculate_network_metrics()
    test_empty_graph_handling()
    test_calculate_network_metrics()
    test_visualize_graph()

    print("\nAll tests passed!")

run_all_tests()